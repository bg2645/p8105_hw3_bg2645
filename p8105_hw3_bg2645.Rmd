---
title: "p8105_hw3_bg2645"
author: "Bing Bing Guo"
date: "10/9/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Question 1: 
```{r} 
library(tidyverse)
library(p8105.datasets)
library(dplyr)
library(viridis)
library(ggridges) 
library(patchwork)
data("instacart")
```

* There were `r nrow(instacart)` observations and `r ncol(instacart)` variables in the `instacart` dataset. 
* The key variables in the `instacart` dataset were the porducts that were ordered, the order in which they were added into the cart, if this item has previously been reordered, what time and day of week the order was placed, and the aisle and department for each product, and the unique user_id for each customer. 
*Thus, based on the observations you can see that for example, on what day of the which and on which hour did they place an order for bulgarian yoghurt, which aisle and department this item belongs to, if they had previously ordered it before and how many days it had been since their last order, and in what order they had put this item in their cart. 

```{r} 
instacart %>%
  count(aisle_id, name = "n")%>%
arrange(desc(n))
```
* There are 134 aisles in instacart, the most ordered items are from aisle 83, 24, and 123 respectively - in which aisle 83 had the most ordered items out out of all the aisles. 


```{r}
instacart %>%
group_by(aisle) %>%
summarize(n_aisle = n()) %>%
filter(n_aisle> 10000) %>%
arrange((desc(n_aisle))) %>%
ggplot(aes(x = aisle , y = n_aisle,color=aisle)) + geom_point() + 
labs(
    title = "Number of Items Ordered in Each Aisle", 
    x = "Aisle",
    y = "Number of Items Ordered (n)",
    caption = "This plot shows the number of items ordered in each aisle, limited to aisles with more than 10,000 items ordered" ) + 
viridis::scale_color_viridis(discrete = TRUE) + 
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),   
    plot.caption = element_text(hjust = 0, face = "italic"), 
    legend.position = "none", 
    axis.text.x = element_text(angle=70, hjust=1) ) 
```

```{r} 
top3_data = instacart %>%
filter(aisle %in% c("baking ingredients", "dog food care","packaged vegetables fruits")) %>%
group_by(aisle, product_name) %>%
summarize(n = n()) %>%
top_n(3) %>%
arrange(desc(n)) %>%
knitr::kable()
top3_data
```

```{r} 
instacart %>%
select(product_name, order_dow, order_hour_of_day) %>%
group_by(product_name, order_dow) %>%
summarize(mean_hour = mean(order_hour_of_day)) %>%
mutate(order_dow = recode(order_dow, "0" = "Sunday", "1" = "Monday", "2" = "Tuesday", "3" = "Wednesday", "4" = "Thursday", "5" = "Friday", "6" = "Saturday"))  %>%
filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream"))  %>%
pivot_wider(names_from = "order_dow", values_from = "mean_hour") %>%
knitr::kable(digits = 3)
```
## Question 2: 
```{r}
data("brfss_smart2010") 
```

```{r} 
clean_brfss_data = brfss_smart2010 %>%
janitor::clean_names() %>%
separate(locationdesc, into = c("state", "county"), sep=3) %>% 
mutate(county= stringr::str_replace(county, "- ", "")) %>% 
filter(topic == "Overall Health", 
       response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>%
mutate(response = factor(response, levels = c("Poor","Fair","Good","Very good", "Excellent" ))) %>%
select(-state, -location_id, -data_value_footnote_symbol, -data_value_footnote)
clean_brfss_data
```


```{r} 
clean_brfss_data %>% 
filter(year == 2002) %>%
group_by (locationabbr)  %>%
summarize(n_locations = n_distinct(county)) %>%
filter(n_locations >= 7) %>%
arrange(n_locations) 
```
* Connecticut, Florida, North Carolina, Massachusettes, New Jersey, and Pennslyvania were all observed at 7 or more locations in 2002. Thus, 6 states were observed at 7 or more locations.   
```{r}
clean_brfss_data %>% 
filter(year == 2010) %>%
group_by (locationabbr)  %>%
summarize(n_locations = n_distinct(county)) %>%
filter(n_locations >= 7) %>%
arrange(n_locations)
```
* Colorado, Pennslyvania, South Carolina, Ohio, Massachusettes, New York, Nebraska, Washington, California, Maryland, North Carolina, Texas, New Jersey, and Florida were all observed at 7 or more locations in 2010. Thus 14 states were observed at 7 more more locations.

```{r}
excellent_data = clean_brfss_data %>%
filter(response=="Excellent")%>% 
group_by(year, locationabbr, county) %>% 
summarize(mean_value = mean(data_value)) 
excellent_data
```

```{r}

excellent_data %>%
ggplot(aes(x = year , y = mean_value, color=locationabbr)) + geom_line() + 
labs(
    title = "Average Value Over Time within a State", 
    x = "Year",
    y = "Average Value") + 
viridis::scale_color_viridis(discrete = TRUE) + 
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, size = 14), 
    axis.text.x = element_text(angle=70, hjust=1))
```

Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

```{r} 
combined_plot = 
clean_brfss_data %>%
  filter(topic == "Overall Health",  
         year == "2006" | 
         year == "2010",
         locationabbr == "NY") 
combined_plot

twopanel_plot = combined_plot %>% 
  ggplot(aes(x = response, y = data_value, color = response))+
  geom_boxplot()+
  facet_grid(. ~year) 
twopanel_plot
```

Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State

## Question 3: 
```{r}
accel_data = 
  read_csv("./Data/accel_data.csv") %>% 
janitor::clean_names() %>% 
pivot_longer( 
  activity_1:activity_1440, 
  names_to = "activity_min", 
  values_to = "activity_count") %>%
mutate(activity_min = stringr::str_replace(activity_min, "activity_", ""), activity_min = as.numeric(activity_min), weekday = day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"), weekend = day %in% c("Saturday", "Sunday"), day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) 
accel_data
```

* There were `r nrow(accel_data)` observations and `r ncol(accel_data)` variables in the `accel_data` dataset. The mean activity count was `r mean(pull(accel_data, activity_count))`. 
* The key variables in the `accel_data` dataset was the week, day id, day, activity in minutes, activity count, and variables determining whether the day was a weekday or not. The day variable was a factor variable that was categorized into levels. 

```{r}
total_activity_data = accel_data %>%
  group_by(day) %>%
  summarize(total_activity = sum(activity_count)) %>%
  knitr::kable()
total_activity_data
```

* no trend was observed, however Friday had the highest total_activity value. 
```{r}
accel_data %>%
group_by(day_id, day) %>% 
summarize(total_activity = sum(activity_count)) %>% 
ggplot(aes(x = day_id, y = total_activity, color = day)) + geom_point() + geom_line() +
labs(title = "Activity Time by Day", 
     x = "Day ID",
     y = "Total Activity Time") + 
  viridis::scale_color_viridis(discrete = TRUE) + 
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, size = 14), 
    axis.text.x = element_text(angle=70, hjust=1))
```

* Based on the plot above, total activity time decreases on weekends compared to weekdays. Activity level on weekdays seemed to be more constant across days. There was drastic spike in actvity levels on Monday on day 10 to ~day 16 and there was  also another drastic spike in activity level on Friday from day 22 to 28. 
