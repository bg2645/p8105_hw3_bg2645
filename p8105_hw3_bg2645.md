p8105\_hw3\_bg2645
================
Bing Bing Guo
10/9/2019

## Question 1:

``` r
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──

    ## ✔ ggplot2 3.2.1     ✔ purrr   0.3.2
    ## ✔ tibble  2.1.3     ✔ dplyr   0.8.3
    ## ✔ tidyr   1.0.0     ✔ stringr 1.4.0
    ## ✔ readr   1.3.1     ✔ forcats 0.4.0

    ## ── Conflicts ────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

``` r
library(p8105.datasets)
library(dplyr)
library(viridis)
```

    ## Loading required package: viridisLite

``` r
library(ggridges) 
```

    ## 
    ## Attaching package: 'ggridges'

    ## The following object is masked from 'package:ggplot2':
    ## 
    ##     scale_discrete_manual

``` r
library(patchwork)
data("instacart")
```

  - There were 1384617 observations and 15 variables in the `instacart`
    dataset.
  - The key variables in the `instacart` dataset were the names

<!-- end list -->

``` r
instacart %>%
  count(aisle_id, name = "n")%>%
arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle_id      n
    ##       <int>  <int>
    ##  1       83 150609
    ##  2       24 150473
    ##  3      123  78493
    ##  4      120  55240
    ##  5       21  41699
    ##  6      115  36617
    ##  7       84  32644
    ##  8      107  31269
    ##  9       91  26240
    ## 10      112  23635
    ## # … with 124 more rows

  - There are 134 aisles in instacart, the most ordered items are from
    aisle 83, 24, and 123 respectively - in which aisle 83 had the most
    ordered items out out of all the aisles.

<!-- end list -->

``` r
instacart %>%
group_by(aisle) %>%
summarize(n_aisle = n()) %>%
filter(n_aisle> 10000) %>%
arrange((desc(n_aisle))) %>%
ggplot(aes(x = aisle , y = n_aisle,color=aisle)) + geom_point() + 
labs(
    title = "Number of Items Ordered in Each Aisle", 
    x = "Aisle",
    y = "Number of Items Ordered (n)",
    caption = "This plot shows the number of items ordered in each aisle, limited to aisles with more than 10,000 items ordered" ) + 
viridis::scale_color_viridis(discrete = TRUE) + 
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),   
    plot.caption = element_text(hjust = 0, face = "italic"), 
    legend.position = "none", 
    axis.text.x = element_text(angle=70, hjust=1) ) 
```

![](p8105_hw3_bg2645_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

``` r
top3_data = instacart %>%
filter(aisle %in% c("baking ingredients", "dog food care","packaged vegetables fruits")) %>%
group_by(aisle, product_name) %>%
summarize(n = n()) %>%
top_n(3) %>%
arrange(desc(n)) %>%
knitr::kable()
```

    ## Selecting by n

``` r
top3_data
```

| aisle                      | product\_name                                 |    n |
| :------------------------- | :-------------------------------------------- | ---: |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |
| baking ingredients         | Light Brown Sugar                             |  499 |
| baking ingredients         | Pure Baking Soda                              |  387 |
| baking ingredients         | Cane Sugar                                    |  336 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |
| dog food care              | Small Dog Biscuits                            |   26 |

``` r
instacart %>%
select(product_name, order_dow, order_hour_of_day) %>%
group_by(product_name, order_dow) %>%
summarize(mean_hour = mean(order_hour_of_day)) %>%
mutate(order_dow = recode(order_dow, "0" = "Sunday", "1" = "Monday", "2" = "Tuesday", "3" = "Wednesday", "4" = "Thursday", "5" = "Friday", "6" = "Saturday"))  %>%
filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream"))  %>%
pivot_wider(names_from = "order_dow", values_from = "mean_hour") %>%
knitr::kable(digits = 3)
```

| product\_name    | Sunday | Monday | Tuesday | Wednesday | Thursday | Friday | Saturday |
| :--------------- | -----: | -----: | ------: | --------: | -------: | -----: | -------: |
| Coffee Ice Cream | 13.774 | 14.316 |  15.381 |    15.318 |   15.217 | 12.263 |   13.833 |
| Pink Lady Apples | 13.441 | 11.360 |  11.702 |    14.250 |   11.552 | 12.784 |   11.938 |
| \#\# Question 2: |        |        |         |           |          |        |          |

``` r
data("brfss_smart2010") 
```

``` r
clean_brfss_data = brfss_smart2010 %>% 
janitor::clean_names() %>% 
separate(locationdesc, into = c("state", "county"), sep=3) %>% 
select(-locationabbr) %>% 
mutate(
topic = "Overall Health", 
response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very Good", "Excellent")) )  %>%
filter(response %in% c("Poor", "Fair", "Good", "Very Good", "Excellent")) 
```

    ## Warning: Unknown levels in `f`: Very Good

``` r
clean_brfss_data
```

    ## # A tibble: 8,500 x 23
    ##     year state county class topic question response sample_size data_value
    ##    <int> <chr> <chr>  <chr> <chr> <chr>    <fct>          <int>      <dbl>
    ##  1  2010 "AL " - Jef… Heal… Over… How is … Excelle…          94       18.9
    ##  2  2010 "AL " - Jef… Heal… Over… How is … Good             208       33.1
    ##  3  2010 "AL " - Jef… Heal… Over… How is … Fair             107       12.5
    ##  4  2010 "AL " - Jef… Heal… Over… How is … Poor              45        5.5
    ##  5  2010 "AL " - Mob… Heal… Over… How is … Excelle…          91       15.6
    ##  6  2010 "AL " - Mob… Heal… Over… How is … Good             224       31.2
    ##  7  2010 "AL " - Mob… Heal… Over… How is … Fair             120       15.5
    ##  8  2010 "AL " - Mob… Heal… Over… How is … Poor              66        6.4
    ##  9  2010 "AL " - Tus… Heal… Over… How is … Excelle…          58       20.8
    ## 10  2010 "AL " - Tus… Heal… Over… How is … Good             171       33.8
    ## # … with 8,490 more rows, and 14 more variables:
    ## #   confidence_limit_low <dbl>, confidence_limit_high <dbl>,
    ## #   display_order <int>, data_value_unit <chr>, data_value_type <chr>,
    ## #   data_value_footnote_symbol <chr>, data_value_footnote <chr>,
    ## #   data_source <chr>, class_id <chr>, topic_id <chr>, location_id <chr>,
    ## #   question_id <chr>, respid <chr>, geo_location <chr>

``` r
clean_brfss_data %>% 
filter(year == 2002) %>%
group_by (state)  %>%
summarize(n_locations = n_distinct(county)) %>%
filter(n_locations >= 7) %>%
arrange(n_locations) 
```

    ## # A tibble: 6 x 2
    ##   state n_locations
    ##   <chr>       <int>
    ## 1 "CT "           7
    ## 2 "FL "           7
    ## 3 "NC "           7
    ## 4 "MA "           8
    ## 5 "NJ "           8
    ## 6 "PA "          10

  - Pennslyvania, Massachusetts, New Jersey, Connecticut, Florida, North
    Carolina were all observed at 7 or more locations in 2002.

<!-- end list -->

``` r
clean_brfss_data %>% 
filter(year == 2010) %>%
group_by (state)  %>%
summarize(n_locations = n_distinct(county)) %>%
filter(n_locations >= 7) %>%
arrange(n_locations)
```

    ## # A tibble: 14 x 2
    ##    state n_locations
    ##    <chr>       <int>
    ##  1 "CO "           7
    ##  2 "PA "           7
    ##  3 "SC "           7
    ##  4 "OH "           8
    ##  5 "MA "           9
    ##  6 "NY "           9
    ##  7 "NE "          10
    ##  8 "WA "          10
    ##  9 "CA "          12
    ## 10 "MD "          12
    ## 11 "NC "          12
    ## 12 "TX "          16
    ## 13 "NJ "          19
    ## 14 "FL "          41

  - Florida, New Jersey, Texas, California, Maryland, North Carolina,
    Nebraska, Washington, Massachusettes, New York, Ohio, Colorodo,
    Pennsylvania, South Carolina were all observed at 7 or more
    locations in 2002.

<!-- end list -->

``` r
excellent_data=clean_brfss_data %>%
filter(response=="Excellent")%>% 
group_by(state, response,  year) %>% 
summarize(mean_value = mean(data_value)) 
excellent_data
```

    ## # A tibble: 443 x 4
    ## # Groups:   state, response [51]
    ##    state response   year mean_value
    ##    <chr> <fct>     <int>      <dbl>
    ##  1 "AK " Excellent  2002       27.9
    ##  2 "AK " Excellent  2003       24.8
    ##  3 "AK " Excellent  2004       23.0
    ##  4 "AK " Excellent  2005       23.8
    ##  5 "AK " Excellent  2007       23.5
    ##  6 "AK " Excellent  2008       20.6
    ##  7 "AK " Excellent  2009       23.2
    ##  8 "AL " Excellent  2002       18.5
    ##  9 "AL " Excellent  2003       19.5
    ## 10 "AL " Excellent  2004       20  
    ## # … with 433 more rows

``` r
excellent_data %>%
ggplot(aes(x = year , y = mean_value, color=state)) + geom_line() + 
labs(
    title = "Average Value Over Time within a State", 
    x = "Year",
    y = "Average Value") + 
viridis::scale_color_viridis(discrete = TRUE) + 
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, size = 14), 
    axis.text.x = element_text(angle=70, hjust=1))
```

    ## Warning: Removed 3 rows containing missing values (geom_path).

![](p8105_hw3_bg2645_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->

Make a “spaghetti” plot of this average value over time within a state
(that is, make a plot showing a line for each state across years – the
geom\_line geometry and group aesthetic will help).

Make a two-panel plot showing, for the years 2006, and 2010,
distribution of data\_value for responses (“Poor” to “Excellent”) among
locations in NY State

## Question 3:
