p8105\_hw3\_bg2645
================
Bing Bing Guo
10/9/2019

## Question 1:

``` r
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ──

    ## ✔ ggplot2 3.2.1     ✔ purrr   0.3.2
    ## ✔ tibble  2.1.3     ✔ dplyr   0.8.3
    ## ✔ tidyr   1.0.0     ✔ stringr 1.4.0
    ## ✔ readr   1.3.1     ✔ forcats 0.4.0

    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

``` r
library(p8105.datasets)
library(dplyr)
library(viridis)
```

    ## Loading required package: viridisLite

``` r
library(ggridges) 
```

    ## 
    ## Attaching package: 'ggridges'

    ## The following object is masked from 'package:ggplot2':
    ## 
    ##     scale_discrete_manual

``` r
library(patchwork)
data("instacart")
```

  - There were 1384617 observations and 15 variables in the `instacart`
    dataset.
  - The key variables in the `instacart` dataset were the names

<!-- end list -->

``` r
instacart %>%
  count(aisle_id, name = "n")%>%
arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle_id      n
    ##       <int>  <int>
    ##  1       83 150609
    ##  2       24 150473
    ##  3      123  78493
    ##  4      120  55240
    ##  5       21  41699
    ##  6      115  36617
    ##  7       84  32644
    ##  8      107  31269
    ##  9       91  26240
    ## 10      112  23635
    ## # … with 124 more rows

  - There are 134 aisles in instacart, the most ordered items are from
    aisle 83, 24, and 123 respectively - in which aisle 83 had the most
    ordered items out out of all the aisles.

<!-- end list -->

``` r
instacart %>%
group_by(aisle) %>%
summarize(n_aisle = n()) %>%
filter(n_aisle> 10000) %>%
arrange((desc(n_aisle))) %>%
ggplot(aes(x = aisle , y = n_aisle,color=aisle)) + geom_point() + 
labs(
    title = "Number of Items Ordered in Each Aisle", 
    x = "Aisle",
    y = "Number of Items Ordered (n)",
    caption = "This plot shows the number of items ordered in each aisle, limited to aisles with more than 10,000 items ordered" ) + 
viridis::scale_color_viridis(discrete = TRUE) + 
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),   
    plot.caption = element_text(hjust = 0, face = "italic"), 
    legend.position = "none", 
    axis.text.x = element_text(angle=70, hjust=1) ) 
```

![](p8105_hw3_bg2645_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

``` r
top3_data = instacart %>%
filter(aisle %in% c("baking ingredients", "dog food care","packaged vegetables fruits")) %>%
group_by(aisle, product_name) %>%
summarize(n = n()) %>%
top_n(3) %>%
arrange(desc(n)) %>%
knitr::kable()
```

    ## Selecting by n

``` r
top3_data
```

| aisle                      | product\_name                                 |    n |
| :------------------------- | :-------------------------------------------- | ---: |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |
| baking ingredients         | Light Brown Sugar                             |  499 |
| baking ingredients         | Pure Baking Soda                              |  387 |
| baking ingredients         | Cane Sugar                                    |  336 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |
| dog food care              | Small Dog Biscuits                            |   26 |

``` r
instacart %>%
select(product_name, order_dow, order_hour_of_day) %>%
group_by(product_name, order_dow) %>%
summarize(mean_hour = mean(order_hour_of_day)) %>%
mutate(order_dow = recode(order_dow, "0" = "Sunday", "1" = "Monday", "2" = "Tuesday", "3" = "Wednesday", "4" = "Thursday", "5" = "Friday", "6" = "Saturday"))  %>%
filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream"))  %>%
pivot_wider(names_from = "order_dow", values_from = "mean_hour") %>%
knitr::kable(digits = 3)
```

| product\_name    | Sunday | Monday | Tuesday | Wednesday | Thursday | Friday | Saturday |
| :--------------- | -----: | -----: | ------: | --------: | -------: | -----: | -------: |
| Coffee Ice Cream | 13.774 | 14.316 |  15.381 |    15.318 |   15.217 | 12.263 |   13.833 |
| Pink Lady Apples | 13.441 | 11.360 |  11.702 |    14.250 |   11.552 | 12.784 |   11.938 |
| \#\# Question 2: |        |        |         |           |          |        |          |

``` r
data("brfss_smart2010") 
```

``` r
clean_brfss_data = brfss_smart2010 %>% 
janitor::clean_names() %>% 
separate(locationdesc, into = c("state", "county"), sep=3) %>% 
mutate(county= stringr::str_replace(county, "- ", "")) %>% 
select(-locationabbr, -location_id, -data_value_footnote_symbol, -data_value_footnote) %>% 
filter(topic == "Overall Health")%>% 
mutate(response = forcats::fct_relevel(response, c("Poor", "Fair", "Good", "Very Good", "Excellent")))  
```

    ## Warning: Unknown levels in `f`: Very Good

``` r
clean_brfss_data
```

    ## # A tibble: 10,625 x 20
    ##     year state county class topic question response sample_size data_value
    ##    <int> <chr> <chr>  <chr> <chr> <chr>    <fct>          <int>      <dbl>
    ##  1  2010 "AL " Jeffe… Heal… Over… How is … Excelle…          94       18.9
    ##  2  2010 "AL " Jeffe… Heal… Over… How is … Very go…         148       30  
    ##  3  2010 "AL " Jeffe… Heal… Over… How is … Good             208       33.1
    ##  4  2010 "AL " Jeffe… Heal… Over… How is … Fair             107       12.5
    ##  5  2010 "AL " Jeffe… Heal… Over… How is … Poor              45        5.5
    ##  6  2010 "AL " Mobil… Heal… Over… How is … Excelle…          91       15.6
    ##  7  2010 "AL " Mobil… Heal… Over… How is … Very go…         177       31.3
    ##  8  2010 "AL " Mobil… Heal… Over… How is … Good             224       31.2
    ##  9  2010 "AL " Mobil… Heal… Over… How is … Fair             120       15.5
    ## 10  2010 "AL " Mobil… Heal… Over… How is … Poor              66        6.4
    ## # … with 10,615 more rows, and 11 more variables:
    ## #   confidence_limit_low <dbl>, confidence_limit_high <dbl>,
    ## #   display_order <int>, data_value_unit <chr>, data_value_type <chr>,
    ## #   data_source <chr>, class_id <chr>, topic_id <chr>, question_id <chr>,
    ## #   respid <chr>, geo_location <chr>

``` r
clean_brfss_data %>% 
filter(year == 2002) %>%
group_by (state)  %>%
summarize(n_locations = n_distinct(county)) %>%
filter(n_locations >= 7) %>%
arrange(n_locations) 
```

    ## # A tibble: 6 x 2
    ##   state n_locations
    ##   <chr>       <int>
    ## 1 "CT "           7
    ## 2 "FL "           7
    ## 3 "NC "           7
    ## 4 "MA "           8
    ## 5 "NJ "           8
    ## 6 "PA "          10

  - Pennslyvania, Massachusetts, New Jersey, Connecticut, Florida, North
    Carolina were all observed at 7 or more locations in 2002.

<!-- end list -->

``` r
clean_brfss_data %>% 
filter(year == 2010) %>%
group_by (state)  %>%
summarize(n_locations = n_distinct(county)) %>%
filter(n_locations >= 7) %>%
arrange(n_locations)
```

    ## # A tibble: 14 x 2
    ##    state n_locations
    ##    <chr>       <int>
    ##  1 "CO "           7
    ##  2 "PA "           7
    ##  3 "SC "           7
    ##  4 "OH "           8
    ##  5 "MA "           9
    ##  6 "NY "           9
    ##  7 "NE "          10
    ##  8 "WA "          10
    ##  9 "CA "          12
    ## 10 "MD "          12
    ## 11 "NC "          12
    ## 12 "TX "          16
    ## 13 "NJ "          19
    ## 14 "FL "          41

  - Florida, New Jersey, Texas, California, Maryland, North Carolina,
    Nebraska, Washington, Massachusettes, New York, Ohio, Colorodo,
    Pennsylvania, South Carolina were all observed at 7 or more
    locations in 2002.

<!-- end list -->

``` r
excellent_data = clean_brfss_data %>%
filter(response=="Excellent")%>% 
group_by(year, state, county) %>% 
summarize(mean_value = mean(data_value)) 
excellent_data
```

    ## # A tibble: 2,125 x 4
    ## # Groups:   year, state [443]
    ##     year state county                 mean_value
    ##    <int> <chr> <chr>                       <dbl>
    ##  1  2002 "AK " Anchorage Municipality       27.9
    ##  2  2002 "AL " Jefferson County             18.5
    ##  3  2002 "AR " Pulaski County               24.1
    ##  4  2002 "AZ " Maricopa County              21.6
    ##  5  2002 "AZ " Pima County                  26.6
    ##  6  2002 "CA " Los Angeles County           22.7
    ##  7  2002 "CO " Adams County                 21.2
    ##  8  2002 "CO " Arapahoe County              25.5
    ##  9  2002 "CO " Denver County                22.2
    ## 10  2002 "CO " Jefferson County             23.4
    ## # … with 2,115 more rows

``` r
excellent_data %>%
ggplot(aes(x = year , y = mean_value, color=state)) + geom_line() + 
labs(
    title = "Average Value Over Time within a State", 
    x = "Year",
    y = "Average Value") + 
viridis::scale_color_viridis(discrete = TRUE) + 
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, size = 14), 
    axis.text.x = element_text(angle=70, hjust=1))
```

![](p8105_hw3_bg2645_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->

Make a “spaghetti” plot of this average value over time within a state
(that is, make a plot showing a line for each state across years – the
geom\_line geometry and group aesthetic will help).

``` r
plot_2006 = 
clean_brfss_data %>%
  filter(state == "NY", year == "2006") %>%
  ggplot(aes(x = response, y = data_value, color = response))+
  geom_point(alpha = 0.5)+
  labs(
    title = "NY State Distribution in 2006",
    x = "Responses",
    y = "Data Values") 
plot_2006
```

![](p8105_hw3_bg2645_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->

Make a two-panel plot showing, for the years 2006, and 2010,
distribution of data\_value for responses (“Poor” to “Excellent”) among
locations in NY State

## Question 3:

``` r
accel_data = 
  read_csv("./Data/accel_data.csv") %>% 
janitor::clean_names() %>% 
pivot_longer( 
  activity_1:activity_1440, 
  names_to = "activity_min", 
  values_to = "activity_count") %>%
mutate(activity_min = stringr::str_replace(activity_min, "activity_", "")) %>% 
mutate(activity_min = as.numeric(activity_min)) %>%
mutate(weekday = day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"), weekend = day %in% c("Saturday", "Sunday"))
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

``` r
accel_data
```

    ## # A tibble: 50,400 x 7
    ##     week day_id day    activity_min activity_count weekday weekend
    ##    <dbl>  <dbl> <chr>         <dbl>          <dbl> <lgl>   <lgl>  
    ##  1     1      1 Friday            1           88.4 TRUE    FALSE  
    ##  2     1      1 Friday            2           82.2 TRUE    FALSE  
    ##  3     1      1 Friday            3           64.4 TRUE    FALSE  
    ##  4     1      1 Friday            4           70.0 TRUE    FALSE  
    ##  5     1      1 Friday            5           75.0 TRUE    FALSE  
    ##  6     1      1 Friday            6           66.3 TRUE    FALSE  
    ##  7     1      1 Friday            7           53.8 TRUE    FALSE  
    ##  8     1      1 Friday            8           47.8 TRUE    FALSE  
    ##  9     1      1 Friday            9           55.5 TRUE    FALSE  
    ## 10     1      1 Friday           10           43.0 TRUE    FALSE  
    ## # … with 50,390 more rows

``` r
total_activity_data = accel_data %>%
  group_by(day) %>%
  summarize(total_activity = mean(activity_count)) %>%
  arrange(desc(total_activity))
total_activity_data 
```

    ## # A tibble: 7 x 2
    ##   day       total_activity
    ##   <chr>              <dbl>
    ## 1 Friday              318.
    ## 2 Wednesday           296.
    ## 3 Thursday            290.
    ## 4 Sunday              267.
    ## 5 Monday              258.
    ## 6 Tuesday             250.
    ## 7 Saturday            190.

Make a single-panel plot that shows the 24-hour activity time courses
for each day and use color to indicate day of the week.
